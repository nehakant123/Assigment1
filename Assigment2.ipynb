{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83a3558-5df1-4b5f-a4a6-afd5238ddd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Identity function (already implemented).\n",
    "    \"\"\"\n",
    "\n",
    "    # This class is a gimme as it is already implemented for you as an example\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state = x\n",
    "        return x\n",
    "\n",
    "    def derivative(self):\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab44292-bc97-4bb4-a844-d56ff9dde026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Sigmoid non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    # Remember do not change the function signatures as those are needed\n",
    "    # to stay the same for AutoLab.\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state=(1/(1+np.exp(-x)))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return (self.state*(1-self.state))#returning derivative of sigmoid function\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c5a96-a08f-46fa-8357-244d837b82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    Tanh non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        return (1-(self.state*self.state))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef675b5-5f4e-4fdc-9993-bccb0a0fdabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "\n",
    "    \"\"\"\n",
    "    ReLU non-linearity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.state=(np.maximum(0,x))\n",
    "        return self.state\n",
    "\n",
    "    def derivative(self):\n",
    "        relu=(self.state>0,1,0)\n",
    "        return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa33c4-6157-4771-a39f-248237fce895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Criterion):\n",
    "    \"\"\"\n",
    "    Softmax loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftmaxCrossEntropy, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, 10)\n",
    "            y (np.array): (batch size, 10)\n",
    "        Return:\n",
    "            out (np.array): (batch size, )\n",
    "        \"\"\"\n",
    "        self.logits = x\n",
    "        self.labels = y\n",
    "        maximum=np.maximum(self.logits,axis=1).reshape(-1,1)#calculating the maximum value to stabilise the exponential term\n",
    "        sub=self.logits-maximum\n",
    "        self.expl=np.exp(sub)\n",
    "        sum=self.expl.sum(axis=1).reshape(-1,1)#computing the sum of all e^(logits-maximum) values\n",
    "        ans=-(self.logits*self.labels).sum(axis=1)+(maximum+np.log(sum)).reshape(-1,1)#applying cross entropy\n",
    "        self.sig=self.logits/sum\n",
    "\n",
    "        return ans\n",
    "\n",
    "    def derivative(self):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            out (np.array): (batch size, 10)\n",
    "        \"\"\"\n",
    "        return self.sig-self.logits #computing derivative \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5eb0e8-3b2e-40e2-8dbb-ff89e5334746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_feature, out_feature, weight_init_fn, bias_init_fn):\n",
    "\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            W (np.array): (in feature, out feature)\n",
    "            dW (np.array): (in feature, out feature)\n",
    "            momentum_W (np.array): (in feature, out feature)\n",
    "\n",
    "            b (np.array): (1, out feature)\n",
    "            db (np.array): (1, out feature)\n",
    "            momentum_B (np.array): (1, out feature)\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = weight_init_fn(in_feature, out_feature)\n",
    "        self.b = bias_init_fn(out_feature)\n",
    "\n",
    "        # TODO: Complete these but do not change the names.\n",
    "        self.dW = np.zeros(None)\n",
    "        self.db = np.zeros(None)\n",
    "\n",
    "        self.momentum_W = np.zeros(None)\n",
    "        self.momentum_b = np.zeros(None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x=x\n",
    "        ans=np.matmul(self.W,self.x)+self.b #calculated the value of z which is used by the acitivation to calculate probability.Using\n",
    "        return ans                          #np.matmul to calculate its dot product;\n",
    "\n",
    "    def backward(self, delta):\n",
    "        self.dW=np.dot((self.x).T,delta)/delta.shape[0]#here we are calculating change in W and we are dividing by the batch size to get \n",
    "        self.db=np.sum(delta,axis=0)/delta.shape[0]#average gradient.\n",
    "        dx=np.dot(delta,self.W.T)\n",
    "        return dx\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42402b74-f5a5-4390-ba78-00968c085fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple multilayer perceptron\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, hiddens, activations, weight_init_fn,\n",
    "                 bias_init_fn, criterion, lr):\n",
    "\n",
    "        # Don't change this -->\n",
    "        self.train_mode = True\n",
    "        self.nlayers = len(hiddens) + 1\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activations = activations\n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        # <---------------------\n",
    "\n",
    "        # Don't change the name of the following class attributes,\n",
    "        # the autograder will check against these attributes. But you will need to change\n",
    "        # the values in order to initialize them correctly\n",
    "\n",
    "        # Initialize and add all your linear layers into the list 'self.linear_layers'\n",
    "        # (HINT: self.foo = [ bar(???) for ?? in ? ])\n",
    "        # (HINT: Can you use zip here?)\n",
    "        self.linear_layers = [Linear(inf,outf,weight_init_fn,bias_init_fn) for inf ,outf in zip([self.input_size]+hiddens,hiddens+\n",
    "                                                                                                [self.output_size])]\n",
    "        #initialised linear_layers\n",
    "        #zip is used to create pairs. The first pair is (input_size,hiddens[0]) and the last pair is (hiddens[-1],output_size).\n",
    "        #([self.input_size]+hiddens,hiddens+[self.output_size]) is used to create an input list and output list.\n",
    "                                                                                                \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "            x (np.array): (batch size, input_size)\n",
    "        Return:\n",
    "            out (np.array): (batch size, output_size)\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.linear_layers):#enumerate is used to get the value of i and layer element in single step\n",
    "            x=layer(x)#propogating input through each layer of neural network\n",
    "            x=self.activations[i](x)#applying activations on the output of the linear layer to get probabilities \n",
    "            return x\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for layer in self.linear_layers:# here we are reseting the values of gradient to zero to prevent gradient accumulation \n",
    "            layer.dw.fill(0.0)\n",
    "            layer.db.fill(0.0)\n",
    "\n",
    "    def step(self):\n",
    "        for i in range (len(self.linear_layers)):\n",
    "            layer=self.linear_layers[i]\n",
    "            layer.w=layer.w-self.lr*layer.dw#here we are calculating the updated values of weight a biases\n",
    "            layer.b=layer.b-self.lr*layer.db\n",
    "\n",
    "    def backward(self, labels):\n",
    "        final_layer=self.activations[-1]#used to select the activation function for the last layer's output\n",
    "        final_outputs=final_layer.state\n",
    "        loss=self.criterion(final_outputs,labels)#calculating loss\n",
    "        delta=self.criterion.derivative()#calculating delta\n",
    "        for i in range(self.nlayers-1,-1,-1):#iterating over the remaining layers in reverse order\n",
    "            delta=delta*self.activations[i].derative()#computes the derivative of the activation function with respect to its intput value and\n",
    "            delta=self.linear_layers[i].backward(delta)#multiplies it with the gradient.The next line backward function computes the gradient\n",
    "                                                       #of loss with respect to input layer\n",
    "\n",
    "    def error(self, labels):\n",
    "        return (np.argmax(self.output, axis = 1) != np.argmax(labels, axis = 1)).sum()\n",
    "\n",
    "    def total_loss(self, labels):\n",
    "        return self.criterion(self.output, labels).sum()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
